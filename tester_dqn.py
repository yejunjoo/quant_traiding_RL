import yfinance as yf
import gymnasium as gym
import torch
import matplotlib.pyplot as plt
import pickle
import numpy as np
from matplotlib.lines import Line2D
import os

from Environment import StockTradingEnv
from algo.dqn import DQN  # [ë³€ê²½] PPO Actor ëŒ€ì‹  DQN ì„í¬íŠ¸

# ==========================================
# [ì„¤ì •] í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° (ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ í™•ì¸ í•„ìˆ˜)
# ==========================================
# ì‹¤í–‰ ì „ runs/ í´ë” ë‚´ì˜ ì‹¤ì œ ëª¨ë¸ í´ë”ëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.
MODEL_NAME = "StockTrading_PPO_20251217-XXXXXX"
STEP = "10000" # ì €ì¥ëœ ìŠ¤í… ë²ˆí˜¸ (ì˜ˆ: 10000, 20000...)

# DQNì€ step ë‹¨ìœ„ë¡œ ì €ì¥ë¨
MODEL_PATH = f"saved_models/{MODEL_NAME}/dqn_step_{STEP}.pth"
STATS_PATH = f"saved_models/{MODEL_NAME}/obs_rms_step_{STEP}.pkl"

Tickers_candidate = ["AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "TSLA", "META"]
START_DATE = "2023-01-01"
END_DATE = "2024-01-01"
N_tickers = 1

BANKRUPT_COEF = 0.3
TERMINATION_REWARD = -1.0
MAX_BALANCE = 1e7
BALANCE_RAND = False
device = 'cuda' if torch.cuda.is_available() else 'cpu'
MAX_TRADE = 50
# ==========================================

def shape_data_matrix(tickers, start, end):
    print(f"ğŸ“¥ {tickers} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    df = yf.download(tickers, start=start, end=end, auto_adjust=True)

    if len(tickers) == 1:
        raw_data = df.values
        n_days = raw_data.shape[0]
        n_features = raw_data.shape[1]
        data_matrix = raw_data.reshape(n_days, 1, n_features)
    else:
        df_stacked = df.stack(level=1, future_stack=True)
        df_stacked = df_stacked.sort_index(level=[0, 1])
        raw_data = df_stacked.values
        n_days = len(df.index)
        n_features = raw_data.shape[1]
        data_matrix = raw_data.reshape(n_days, len(tickers), n_features)

    print(f"âœ… Data matrix shape: {data_matrix.shape}")
    return data_matrix

def make_env_for_test(data_matrix, balance_rand, bankrupt_coef, termination_reward, max_balance, max_trade, stats_path):
    env = StockTradingEnv(df_matrix=data_matrix,
                          balance_rand=balance_rand,
                          bankrupt_coef=bankrupt_coef,
                          termination_reward=termination_reward,
                          max_trade=max_trade,
                          max_balance=max_balance)
    env = gym.wrappers.RecordEpisodeStatistics(env)
    env = gym.wrappers.FlattenObservation(env)
    env = gym.wrappers.NormalizeObservation(env)

    # [ì¤‘ìš”] í•™ìŠµ ë•Œ ì €ì¥í•œ í†µê³„ê°’(obs_rms) ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(stats_path):
        with open(stats_path, "rb") as f:
            loaded_obs_rms = pickle.load(f)
        env.obs_rms = loaded_obs_rms
        print(f"âœ… Loaded observation statistics from {stats_path}")
    else:
        print(f"âš ï¸ Warning: Stats file not found at {stats_path}. Running without stats load.")

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì • (í†µê³„ ì—…ë°ì´íŠ¸ ì¤‘ì§€)
    env.training = False

    env = gym.wrappers.NormalizeReward(env)

    # [ì¤‘ìš”] Runnerì™€ ë™ì¼í•˜ê²Œ RescaleAction ì ìš© (-1~1 -> -50~50)
    env = gym.wrappers.RescaleAction(env, min_action=-1.0, max_action=1.0)
    env = gym.wrappers.ClipAction(env)
    return env


def test():
    # 1. ë°ì´í„° ì¤€ë¹„
    data_matrix = shape_data_matrix(Tickers_candidate[0:N_tickers], START_DATE, END_DATE)

    # 2. í™˜ê²½ ìƒì„±
    env = make_env_for_test(data_matrix=data_matrix,
                            balance_rand=BALANCE_RAND,
                            bankrupt_coef=BANKRUPT_COEF,
                            termination_reward=TERMINATION_REWARD,
                            max_balance=MAX_BALANCE,
                            max_trade=MAX_TRADE,
                            stats_path=STATS_PATH)

    obs_shape = env.observation_space.shape[0]

    # 3. DQN ì—ì´ì „íŠ¸ ìƒì„± ë° ë¡œë“œ
    # DQN í´ë˜ìŠ¤ ì´ˆê¸°í™” (Action Dimì€ 1ì´ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œ Discrete ë§¤í•‘)
    dqn_agent = DQN(obs_dim=obs_shape, action_dim=1)

    try:
        dqn_agent.q_net.load_state_dict(torch.load(MODEL_PATH, map_location=device))
        print(f"âœ… Loaded pre-trained model from \n{MODEL_PATH}")
    except FileNotFoundError:
        print(f"âŒ Error: Model file not found at {MODEL_PATH}")
        return

    dqn_agent.q_net.eval() # í‰ê°€ ëª¨ë“œ (Dropout ë“± ë¹„í™œì„±í™”)

    # 4. í…ŒìŠ¤íŠ¸ ë£¨í”„ ì¤€ë¹„
    raw_env = env.unwrapped
    obs, info = env.reset()
    done = False

    current_balances = [raw_env.curr_balance]
    rewards = []
    policy_actions = []
    stock_prices_obs = []

    stock_prices_gt = data_matrix[:,0,0] # Ground Truth Prices

    print("ğŸš€ Start Testing ...")

    while not done:
        # [ë³€ê²½] DQN Action ê²°ì • (Eval Mode=Trueë¡œ Epsilon Greedy ë”)
        # dqn_agent.act returns (continuous_action, action_idx)
        action_continuous, action_idx = dqn_agent.act(obs, eval_mode=True)

        # ê¸°ë¡: í˜„ì¬ ì£¼ê°€
        current_price = raw_env.obs_dict['market'][0] if hasattr(raw_env, 'obs_dict') else 0
        stock_prices_obs.append(current_price)

        # ê¸°ë¡: í–‰ë™ (ì‹¤ì œ ë¬¼ëŸ‰ìœ¼ë¡œ ë³€í™˜)
        # RescaleActionì´ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ action_continuous(-1~1) * MAX_TRADEê°€ ì‹¤ì œ ë¬¼ëŸ‰ê³¼ ë¹„ë¡€í•¨
        policy_actions.append(action_continuous[0] * MAX_TRADE)

        # í™˜ê²½ ì§„í–‰ (ì—°ì†ê°’ ì „ë‹¬)
        next_obs, reward, truncated, terminated, info = env.step(action_continuous)

        # ê¸°ë¡: ì”ê³  ë° ë³´ìƒ
        current_balances.append(raw_env.curr_balance)
        rewards.append(raw_env.reward)

        obs = next_obs
        done = terminated or truncated

    print("ğŸ Done Testing.")

    # ê·¸ë˜í”„ë¥¼ ìœ„í•œ ë°ì´í„° ì •ë¦¬
    # current_balancesëŠ” ì´ˆê¸°ê°’ì´ ìˆì–´ 1ê°œ ë” ë§ìœ¼ë¯€ë¡œ ë§ˆì§€ë§‰ ìŠ¤í… ì œì™¸í•˜ê±°ë‚˜ ê¸¸ì´ë¥¼ ë§ì¶¤
    if len(current_balances) > len(rewards):
        current_balances = current_balances[:-1] # ê¸¸ì´ë¥¼ ë§ì¶¤

    print(f"Prices: {len(stock_prices_obs)}, Actions: {len(policy_actions)}")
    print(f"Rewards: {len(rewards)}, Balances: {len(current_balances)}")

    # =========================================================
    # 5. ì‹œê°í™” (Visualization)
    # =========================================================
    fig, axes = plt.subplots(4, 1, figsize=(12, 16), sharex=True)
    steps = range(len(current_balances))

    # 1. Portfolio Balance
    ax1 = axes[0]
    total_return = (current_balances[-1] - current_balances[0]) / current_balances[0] * 100
    ax1.set_title(f"1. Portfolio Balance (Total Return: {total_return:.2f}%)", fontweight='bold')
    ax1.plot(steps, current_balances, color='tab:red', linewidth=2)
    ax1.set_ylabel('Balance (Won)')
    ax1.grid(True, alpha=0.3)

    # 2. Reward
    ax2 = axes[1]
    ax2.set_title("2. Step Reward", fontweight='bold')
    ax2.fill_between(steps, rewards, color='gray', alpha=0.5)
    ax2.plot(steps, rewards, color='black', linewidth=0.5, alpha=0.3)
    ax2.set_ylabel('Reward')
    ax2.grid(True, alpha=0.3)

    # 3. Stock Prices
    ax3 = axes[2]
    ax3.set_title("3. Stock Prices (Ground Truth vs Observed)", fontweight='bold')
    sliced_gt = stock_prices_gt[:len(steps)]
    ax3.plot(steps, sliced_gt, color='black', linestyle='--', label='Ground Truth')
    ax3.plot(steps, stock_prices_obs, color='tab:blue', label='Observed')
    ax3.set_ylabel('Price')
    ax3.legend(loc='upper left')
    ax3.grid(True, alpha=0.3)

    # 4. Policy Actions
    ax4 = axes[3]
    ax4.set_title("4. Agent Actions (Buy/Sell Volume)", fontweight='bold')

    # ë§¤ìˆ˜(ì´ˆë¡)/ë§¤ë„(ë¹¨ê°•) ìƒ‰ìƒ ì§€ì •
    action_colors = ['green' if x > 0 else 'red' if x < 0 else 'gray' for x in policy_actions]

    ax4.bar(steps, policy_actions, color=action_colors, width=1.0)
    ax4.axhline(0, color='black', linewidth=0.8) # 0 ê¸°ì¤€ì„ 
    ax4.set_ylabel('Volume')
    ax4.set_xlabel('Steps')
    ax4.grid(True, alpha=0.3)

    legend_elements = [Line2D([0], [0], color='green', lw=4, label='Buy'),
                       Line2D([0], [0], color='red', lw=4, label='Sell'),
                       Line2D([0], [0], color='gray', lw=4, label='Hold')]
    ax4.legend(handles=legend_elements, loc='upper left')

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    test()